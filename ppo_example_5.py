import ctypes
import numpy as np
import matplotlib.pyplot as plt
import torch
from pytorch_ppo_controller import PPOController  # ç¡®ä¿ä½ å·²å®ç°è¯¥ç±»
import os
import time
import multiprocessing as mp
import copy

# --- é…ç½®å‚æ•° ---
DLL_PATH = r'c:\ç ”ç©¶ç”Ÿ\03è®¾å¤‡ä¿¡æ¯\myRIO\DLLæ–‡ä»¶ - å‚æ•°\Dll3\x64\Debug\Dll1.dll'  # DLLæ–‡ä»¶è·¯å¾„
TOTAL_TRAINING_STEPS = 8000000  # æ€»è®­ç»ƒæ­¥æ•°
STEPS_PER_EPISODE = 6000  # æ¯ä¸ªepisodeçš„æ­¥æ•°
TIME_STEP = 0.02  # æ—¶é—´æ­¥é•¿
SAVE_PLOT_EVERY_EPISODES = 50  # æ¯éš”å¤šå°‘ä¸ªepisodeä¿å­˜ä¸€æ¬¡å›¾åƒ
PLOT_DIR = "episode_plots_ppo_mp"  # å›¾åƒä¿å­˜ç›®å½•
RESULTS_FILENAME = 'ppo_mp_results.csv'  # ç»“æœæ–‡ä»¶å
REWARD_PLOT_FILENAME = 'ppo_mp_rewards_plot.png'  # å¥–åŠ±æ›²çº¿å›¾åƒæ–‡ä»¶å
MODEL_SAVE_PATH = 'ppo_mp_model.pth'  # æ¨¡å‹ä¿å­˜è·¯å¾„
NAN_PENALTY = -1000.0  # NaNå€¼æƒ©ç½š
XY_ANGLE_THRESHOLD = 1.0  # XYæ¬§æ‹‰è§’ç¨³å®šé˜ˆå€¼ï¼ˆåº¦ï¼‰
XY_STABLE_REWARD = 150.0  # XYæ¬§æ‹‰è§’ç¨³å®šå¥–åŠ±åŸºå‡†å€¼
ANGULAR_VEL_THRESHOLD = 50.0  # è§’é€Ÿåº¦ç¨³å®šé˜ˆå€¼ï¼ˆåº¦/ç§’ï¼‰
ANGULAR_VEL_STABLE_REWARD = 120.0  # è§’é€Ÿåº¦ç¨³å®šå¥–åŠ±åŸºå‡†å€¼
NUM_ENVIRONMENTS = 30  # å¹¶è¡Œç¯å¢ƒæ•°é‡
UPDATE_TIMESTEPS = STEPS_PER_EPISODE * NUM_ENVIRONMENTS  # æ›´æ–°ç­–ç•¥çš„æ­¥æ•°
# PPO è¶…å‚æ•°
LR_ACTOR = 0.0003  # Actorå­¦ä¹ ç‡
LR_CRITIC = 0.001  # Criticå­¦ä¹ ç‡
GAMMA = 0.99  # æŠ˜æ‰£å› å­
K_EPOCHS = 10  # æ›´æ–°å‘¨æœŸå†…çš„è¿­ä»£æ¬¡æ•°
EPS_CLIP = 0.2  # PPOçš„è£å‰ªèŒƒå›´
ACTION_STD_INIT = 0.1  # åˆå§‹åŠ¨ä½œæ ‡å‡†å·®
MAX_GRAD_NORM = 0.5  # æœ€å¤§æ¢¯åº¦èŒƒæ•°
# Global simulation/environment parameters (used by workers and PPO controller)
INPUT_SIZE_CONST = 6  # åŠ¨ä½œç©ºé—´ç»´åº¦ï¼Œä¹Ÿæ˜¯çŠ¶æ€çš„ä¸€éƒ¨åˆ†
OUTPUT_SIZE_CONST = 6  # DLLè¾“å‡ºç»´åº¦
EULER_DEADZONE = 360  # æ¬§æ‹‰è§’æ­»åŒº
ANGULAR_VELOCITY_DEADZONE = 100  # è§’é€Ÿåº¦æ­»åŒº
MAX_OUTPUT_VALUE = 1e3  # æœ€å¤§è¾“å‡ºå€¼
# CUDAè®¾ç½®
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
os.makedirs(PLOT_DIR, exist_ok=True)
# è®¾ç½® Matplotlib æ”¯æŒä¸­æ–‡æ˜¾ç¤º
try:
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"è®¾ç½®ä¸­æ–‡å­—ä½“å¤±è´¥: {e}. ç»˜å›¾å¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºä¸­æ–‡ã€‚")
# å…¨å±€å˜é‡ï¼Œç”¨äºä¸»è¿›ç¨‹ç»Ÿè®¡
main_process_episode_count = 0
main_process_total_steps = 0
all_episode_rewards = []  # ç”¨äºç»˜åˆ¶å¹³å‡å¥–åŠ±æ›²çº¿

# --- å·¥ä½œè¿›ç¨‹å‡½æ•° ---
"""
PPO (Proximal Policy Optimization) å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°

æœ¬æ–‡ä»¶å®ç°äº†ä¸€ä¸ªåŸºäºPPOç®—æ³•çš„æ§åˆ¶å™¨ï¼Œç”¨äºé€šè¿‡DLLæ¥å£ä¸ç‰©ç†ä»¿çœŸç³»ç»Ÿäº¤äº’ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬:
- å¤šè¿›ç¨‹å¹¶è¡Œè®­ç»ƒå¤šä¸ªç¯å¢ƒ
- é€šè¿‡DLLæ¥å£è°ƒç”¨ç‰©ç†ä»¿çœŸ
- å®ç°PPOç®—æ³•çš„æ ¸å¿ƒè®­ç»ƒé€»è¾‘
- è®°å½•è®­ç»ƒè¿‡ç¨‹å’Œç»“æœ

å…³é”®ç»„ä»¶:
- PPOController: PPOç®—æ³•å®ç°ç±»
- run_single_environment: å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼Œè´Ÿè´£å•ä¸ªç¯å¢ƒçš„ä»¿çœŸ
- å¥–åŠ±å‡½æ•°è®¾è®¡: åŒ…å«æ¬§æ‹‰è§’ç¨³å®šã€è§’é€Ÿåº¦ç¨³å®šç­‰å¥–åŠ±é¡¹

ä½¿ç”¨è¯´æ˜:
1. é…ç½®DLL_PATHæŒ‡å‘æ­£ç¡®çš„DLLæ–‡ä»¶
2. è°ƒæ•´TOTAL_TRAINING_STEPSç­‰è¶…å‚æ•°
3. è¿è¡Œè„šæœ¬å¼€å§‹è®­ç»ƒ

è¾“å‡º:
- è®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±æ›²çº¿
- ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
- æ¯ä¸ªepisodeçš„ä»¿çœŸç»“æœå›¾
----------æ–‡ä»¶ä¿å­˜åœ¨äº†ï¼šC:\ç ”ç©¶ç”Ÿ\03è®¾å¤‡ä¿¡æ¯\myRIO\DLLæ–‡ä»¶ - å‚æ•°\åŠ å…¥å¼ºåŒ–å­¦ä¹ 6\ç®—æ³•æ–‡ä»¶å¤¹\episode_plots_ppo_mp
"""
def run_single_environment(env_id, experience_queue, action_queues, state_queues, worker_args):
    dll_path = worker_args['dll_path']  # DLLæ–‡ä»¶è·¯å¾„
    steps_per_episode = worker_args['steps_per_episode']  # æ¯ä¸ªepisodeçš„æ­¥æ•°
    time_step = worker_args['time_step']  # æ—¶é—´æ­¥é•¿
    input_size = worker_args['input_size']  # è¾“å…¥å¤§å°
    output_size = worker_args['output_size']  # è¾“å‡ºå¤§å°
    nan_penalty = worker_args['nan_penalty']  # NaNæƒ©ç½š
    xy_angle_threshold = worker_args['xy_angle_threshold']  # XYè§’åº¦é˜ˆå€¼
    xy_stable_reward = worker_args['xy_stable_reward']  # XYç¨³å®šå¥–åŠ±
    angular_vel_threshold = worker_args['angular_vel_threshold']  # è§’é€Ÿåº¦é˜ˆå€¼
    angular_vel_stable_reward = worker_args['angular_vel_stable_reward']  # è§’é€Ÿåº¦ç¨³å®šå¥–åŠ±
    euler_deadzone = worker_args['euler_deadzone']  # æ¬§æ‹‰è§’æ­»åŒº
    angular_velocity_deadzone = worker_args['angular_velocity_deadzone']  # è§’é€Ÿåº¦æ­»åŒº
    max_output_value = worker_args['max_output_value']  # æœ€å¤§è¾“å‡ºå€¼

    # åŠ è½½DLL (æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åŠ è½½)
    try:
        # åŠ è½½DLLæ–‡ä»¶å¹¶è®¾ç½®å‡½æ•°å‚æ•°ç±»å‹
        dll = ctypes.CDLL(dll_path)
        # å®šä¹‰simulateå‡½æ•°å‚æ•°ç±»å‹: è¾“å…¥æ•°ç»„æŒ‡é’ˆ, å½“å‰æ—¶é—´, æ—¶é—´æ­¥é•¿, è¾“å‡ºæ•°ç»„æŒ‡é’ˆ
        dll.simulate.argtypes = [
            ctypes.POINTER(ctypes.c_double), ctypes.c_double,
            ctypes.c_double, ctypes.POINTER(ctypes.c_double)
        ]
        # è®¾ç½®å‡½æ•°è¿”å›ç±»å‹ä¸ºNone
        dll.simulate.restype = None
    except Exception as e:
        # æ•è·DLLåŠ è½½å¼‚å¸¸å¹¶è¿”å›
        print(f"[Worker {env_id}] åŠ è½½DLLå¤±è´¥: {e}")
        return

    # æ‰“å°å¼€å§‹è¿è¡Œä¿¡æ¯
    print(f"[Worker {env_id}] å¼€å§‹è¿è¡Œ... ")

    # åˆå§‹åŒ–é›†æ•°è®¡æ•°å™¨
    episode_num = 0

    # æ— é™å¾ªç¯ï¼Œç”¨äºå¤„ç†æ¯ä¸ªé›†æ•°
    while True:
        # å¢åŠ é›†æ•°è®¡æ•°å™¨
        episode_num += 1

        # åˆå§‹åŒ–å½“å‰è¾“å…¥æ•°ç»„ï¼Œå¤§å°ä¸º(1, input_size)ï¼Œæ•°æ®ç±»å‹ä¸ºfloat64
        current_input_array = np.zeros((1, input_size), dtype=np.float64)

        # åˆå§‹åŒ–å½“å‰è¾“å‡ºæ•°ç»„ï¼Œå¤§å°ä¸º(output_size)ï¼Œæ•°æ®ç±»å‹ä¸ºfloat64
        current_output_array = np.zeros(output_size, dtype=np.float64)

        # åˆå§‹åŒ–å‰ä¸€æ¬¡è¾“å‡ºæ•°ç»„ï¼Œå¤§å°ä¸º(output_size)ï¼Œæ•°æ®ç±»å‹ä¸ºfloat64
        prev_output_array_sim = np.zeros(output_size, dtype=np.float64)

        # åˆå§‹åŒ–å‰å‰ä¸€æ¬¡è¾“å‡ºæ•°ç»„ï¼Œå¤§å°ä¸º(output_size)ï¼Œæ•°æ®ç±»å‹ä¸ºfloat64
        prev_prev_output_array_sim = np.zeros(output_size, dtype=np.float64)

        # åˆå§‹åŒ–å½“å‰ä»¿çœŸæ—¶é—´
        current_time_sim = 0.0

        # åˆå§‹åŒ–å½“å‰é›†æ•°çš„æ¬§æ‹‰è§’åˆ—è¡¨
        episode_euler_angles = []

        # åˆå§‹åŒ–å½“å‰é›†æ•°çš„è¾“å…¥åŠ¨ä½œåˆ—è¡¨
        episode_input_actions = []

        # ğŸ“Œã€æ—¥å¿—ä½ç½® 1ã€‘ï¼šåˆå§‹åŒ–æ¬§æ‹‰è§’åˆ—è¡¨
        print(f"[Worker {env_id}] åˆå§‹åŒ– episode_euler_angles (åˆå§‹ä¸ºç©ºåˆ—è¡¨)")

        # æ‰§è¡Œä¸€æ¬¡DLLè°ƒç”¨è·å–åˆå§‹éé›¶çŠ¶æ€
        # è°ƒç”¨DLLçš„simulateå‡½æ•°è¿›è¡Œä»¿çœŸ
        dll.simulate(
            # å°†è¾“å…¥æ•°ç»„è½¬æ¢ä¸ºCåŒç²¾åº¦æµ®ç‚¹æ•°æŒ‡é’ˆ
            current_input_array.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),
            # å½“å‰ä»¿çœŸæ—¶é—´
            current_time_sim,
            # æ—¶é—´æ­¥é•¿
            time_step,
            # å°†è¾“å‡ºæ•°ç»„è½¬æ¢ä¸ºCåŒç²¾åº¦æµ®ç‚¹æ•°æŒ‡é’ˆ
            current_output_array.ctypes.data_as(ctypes.POINTER(ctypes.c_double))
        )

        # æ›´æ–°å½“å‰ä»¿çœŸæ—¶é—´
        current_time_sim += time_step

        # ä¿å­˜å½“å‰è¾“å‡ºæ•°ç»„ä½œä¸ºä¸Šä¸€ä¸ªè¾“å‡ºæ•°ç»„
        prev_output_array_sim = current_output_array.copy()

        # æå–åˆå§‹æ¬§æ‹‰è§’
        initial_euler_angles = current_output_array[3:]

        # åˆå§‹åŒ–è§’é€Ÿåº¦ä¸ºé›¶
        initial_angular_velocities = np.zeros(3)

        # æ„å»ºçŠ¶æ€å‘é‡ï¼ŒåŒ…å«è¾“å…¥ã€æ¬§æ‹‰è§’å’Œè§’é€Ÿåº¦
        state = np.concatenate([
            current_input_array[0], initial_euler_angles, initial_angular_velocities
        ]).flatten()

        # è®°å½•å½“å‰é›†æ•°çš„æ¬§æ‹‰è§’
        episode_euler_angles.append(initial_euler_angles)

        # è®°å½•å½“å‰é›†æ•°çš„è§’é€Ÿåº¦
        episode_angular_velocities = []

        # ğŸ“Œã€æ—¥å¿—ä½ç½® 2ã€‘ï¼šé¦–æ¬¡è°ƒç”¨ DLL åæ·»åŠ åˆå§‹æ¬§æ‹‰è§’
        print(f"[Worker {env_id}] Step 0: åˆå§‹æ¬§æ‹‰è§’ = {initial_euler_angles}")
        last_update_step = 0  # åˆå§‹åŒ–ä¸Šæ¬¡æ›´æ–°çš„æ­¥æ•°
        # éå†æ¯ä¸ªæ—¶é—´æ­¥
        for step in range(steps_per_episode):
            try:
                # å°†çŠ¶æ€æ”¾å…¥é˜Ÿåˆ—ä¸­
                state_queues[env_id].put(state.copy())
            except Exception as e:
                # æ‰“å°å‘é€çŠ¶æ€å¤±è´¥çš„é”™è¯¯ä¿¡æ¯
                print(f"[Worker {env_id}] å‘é€çŠ¶æ€å¤±è´¥: {e}")
                return

            try:
                # ä»é˜Ÿåˆ—ä¸­è·å–åŠ¨ä½œå…ƒç»„ï¼Œè¶…æ—¶æ—¶é—´ä¸º20ç§’
                action_tuple = action_queues[env_id].get(timeout=20)
                # å¦‚æœåŠ¨ä½œå…ƒç»„ä¸ºNoneï¼Œè¿”å›
                if action_tuple is None:
                    return
                # è§£åŒ…åŠ¨ä½œå’ŒåŠ¨ä½œå¯¹æ•°æ¦‚ç‡
                action, action_log_prob = action_tuple
            except mp.queues.Empty:
                # æ‰“å°ç­‰å¾…åŠ¨ä½œè¶…æ—¶çš„é”™è¯¯ä¿¡æ¯
                print(f"[Worker {env_id}] ç­‰å¾…åŠ¨ä½œè¶…æ—¶ï¼Œåœæ­¢ã€‚")
                break
            except Exception as e:
                # æ‰“å°æ¥æ”¶åŠ¨ä½œå¤±è´¥çš„é”™è¯¯ä¿¡æ¯
                print(f"[Worker {env_id}] æ¥æ”¶åŠ¨ä½œå¤±è´¥: {e}")
                break
            
            target_action = current_input_array[0].copy()  # åˆå§‹åŒ–ç›®æ ‡åŠ¨ä½œä¸ºå½“å‰è¾“å…¥
                    # å¯¹åŠ¨ä½œè¿›è¡Œè£å‰ªï¼Œç¡®ä¿å…¶åœ¨æŒ‡å®šèŒƒå›´å†…ï¼Œå¹¶é‡å¡‘ä¸º(1, -1)å½¢çŠ¶
            next_input_array_sim = np.clip(action, [0, 0, 0, -10, -10, 0], [0, 0, 0, 10, 10, 0]).reshape(1, -1)

            # å¦‚æœæ˜¯ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥ï¼Œåˆå§‹åŒ–å‰ä¸€ä¸ªè¾“å…¥æ•°ç»„
            if step == 0:
                prev_input_array_sim = next_input_array_sim.copy()
                last_update_step = step
                target_action = next_input_array_sim[0].copy()
            else:
                # æ¯éš” 10 æ­¥ï¼ˆå³ 0.2 ç§’ï¼‰æ‰å…è®¸æ›´æ–°ä¸€æ¬¡ç›®æ ‡åŠ¨ä½œ
                if (step - last_update_step) >= 5:
                    delta = next_input_array_sim[0] - target_action
                    delta = np.clip(delta, -5.0, 5.0)  # æœ€å¤§å˜åŒ–ä¸è¶…è¿‡ Â±1
                    target_action += delta
                    # print(f"Worker {env_id}: last_update_step = {last_update_step}ï¼Œstep={step}")
                    last_update_step = step
                    # åœ¨last_update_step = stepè¿™è¡Œä»£ç åæ·»åŠ 
                    
                # else:
                #     print(f"æ²¡æ›´æ–°å·®å€¼")

            # åº”ç”¨å½“å‰çš„ç›®æ ‡åŠ¨ä½œ
            next_input_array_sim[0] = target_action.copy()

            # æ›´æ–°å‰ä¸€ä¸ªè¾“å…¥æ•°ç»„ä¸ºå½“å‰è¾“å…¥æ•°ç»„
            prev_input_array_sim = next_input_array_sim.copy()

            # è°ƒç”¨DLLçš„simulateå‡½æ•°è¿›è¡Œç‰©ç†ä»¿çœŸ
            dll.simulate(
                # å°†è¾“å…¥æ•°ç»„è½¬æ¢ä¸ºCåŒç²¾åº¦æµ®ç‚¹æ•°æŒ‡é’ˆ
                current_input_array.ctypes.data_as(ctypes.POINTER(ctypes.c_double)),
                # å½“å‰ä»¿çœŸæ—¶é—´
                current_time_sim,
                # æ—¶é—´æ­¥é•¿
                time_step,
                # å°†è¾“å‡ºæ•°ç»„è½¬æ¢ä¸ºCåŒç²¾åº¦æµ®ç‚¹æ•°æŒ‡é’ˆ
                current_output_array.ctypes.data_as(ctypes.POINTER(ctypes.c_double))
            )

            # # è°ƒè¯•æ—¥å¿—: è®°å½•å½“å‰æ¬§æ‹‰è§’å’Œå®Œæ•´è¾“å‡º
            # print(f"[Worker {env_id}] Step {step}: Euler = {current_output_array[3:]}, Output = {current_output_array}")
            # episode_euler_angles2.append(copy.deepcopy(current_output_array[3:]))  # å°†å½“å‰æ¬§æ‹‰è§’æ·»åŠ åˆ°å½“å‰é›†æ•°çš„æ¬§æ‹‰è§’åˆ—è¡¨ä¸­
            # ğŸ“Œ
            #print(f"[Worker {env_id}] Episode {episode_num} çš„æ¬§æ‹‰è§’æ•°æ® (é•¿åº¦: {len(episode_euler_angles)}):")
            # æ‰“å°æœ€åæ·»åŠ çš„5ä¸ªæ¬§æ‹‰è§’æ•°æ®
            # if len(episode_euler_angles2) >= 5:
            #     print(np.array(episode_euler_angles2[-5:]))
            # else:
            #     print(np.array(episode_euler_angles2))

            # ç¡®ä¿ä»¥ä¸‹ä¸¤è¡Œåœ¨ simulate ä¹‹åæ‰§è¡Œ
            # æå–å½“å‰è¾“å‡ºæ•°ç»„ä¸­çš„æ¬§æ‹‰è§’
            euler_angles_current_sim = current_output_array[3:]
            # print("1TTTTeuler_angles_current_sim: ", euler_angles_current_sim)
            # print("2TTTTprev_output_array_sim: ", prev_output_array_sim[3:] )
            # åˆå§‹åŒ–å¥–åŠ±ä¸º0
            reward = 0
            # åˆå§‹åŒ–å½“å‰æ­¥éª¤æ˜¯å¦ç»ˆæ­¢ä¸ºFalse
            current_step_is_terminal_worker = False

            # æ£€æŸ¥è¾“å‡ºæ•°ç»„ä¸­çš„å€¼æ˜¯å¦è¶…å‡ºæœ€å¤§å€¼æˆ–åŒ…å«NaN
            if np.any(np.abs(current_output_array) >= max_output_value) or np.any(np.isnan(current_output_array)):
                # å¦‚æœè¶…å‡ºæœ€å¤§å€¼æˆ–åŒ…å«NaNï¼Œè®¾ç½®å¥–åŠ±ä¸ºnan_penalty
                reward = nan_penalty
                # è®¾ç½®å½“å‰æ­¥éª¤ç»ˆæ­¢æ ‡å¿—ä¸ºTrue
                current_step_is_terminal_worker = True
            else:
                euler_angles_current_sim = current_output_array[3:]  # æå–å½“å‰æ¨¡æ‹Ÿçš„æ¬§æ‹‰è§’
                angular_velocities_current_sim = (euler_angles_current_sim - prev_output_array_sim[3:]) / time_step  # è®¡ç®—å½“å‰æ¨¡æ‹Ÿçš„è§’é€Ÿåº¦
                episode_euler_angles.append(copy.deepcopy(euler_angles_current_sim))  # å°†å½“å‰æ¬§æ‹‰è§’æ·»åŠ åˆ°å½“å‰é›†æ•°çš„æ¬§æ‹‰è§’åˆ—è¡¨ä¸­

                # ğŸ“Œã€æ—¥å¿—ä½ç½® 4ã€‘ï¼šEpisode ç»“æŸæ—¶æ‰“å°å®Œæ•´æ¬§æ‹‰è§’æ•°æ®
                #print(f"[Worker {env_id}] Episode {episode_num} çš„æ¬§æ‹‰è§’æ•°æ® (é•¿åº¦: {len(episode_euler_angles)}):")
                #print(np.array(episode_euler_angles))
                episode_angular_velocities.append(copy.deepcopy(angular_velocities_current_sim))  # å°†å½“å‰è§’é€Ÿåº¦æ·»åŠ åˆ°å½“å‰é›†æ•°çš„è§’é€Ÿåº¦åˆ—è¡¨ä¸­

                # ğŸ“Œã€æ—¥å¿—ä½ç½® 3ã€‘ï¼šæ¯æ¬¡è°ƒç”¨ DLL åæ‰“å°å½“å‰æ¬§æ‹‰è§’
                #print(f"[Worker {env_id}] Step {step}: Euler = {euler_angles_current_sim}")

                # æ·»åŠ æ¯”è¾ƒé€»è¾‘
               # if step > 0 and np.array_equal(episode_euler_angles[-1], episode_euler_angles[-2]):  # æ£€æŸ¥å½“å‰æ¬§æ‹‰è§’æ˜¯å¦ä¸ä¸Šä¸€æ­¥çš„æ¬§æ‹‰è§’ç›¸åŒ
               #     print(f"[Worker {env_id}] Warning: æ¬§æ‹‰è§’æœªå˜åŒ–ï¼Step {step}")  # å¦‚æœç›¸åŒï¼Œæ‰“å°è­¦å‘Šä¿¡æ¯

               # 1. æ¬§æ‹‰è§’å’Œè§’é€Ÿåº¦ç»å¯¹å€¼æƒ©ç½šï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                euler_penalty = np.sum(np.abs(euler_angles_current_sim))
                angular_vel_penalty = np.sum(np.abs(angular_velocities_current_sim))
                reward -= (euler_penalty + angular_vel_penalty) * 10.0

                # 2. æ”¶æ•›é€Ÿåº¦å¥–åŠ±ï¼ˆæ•°å€¼è¶Šæ¥è¶Šå°ï¼‰
                if step > 0:
                    prev_euler = episode_euler_angles[-2]
                    prev_vel = episode_angular_velocities[-2]
                    euler_improvement = np.sum(np.abs(prev_euler)) - np.sum(np.abs(euler_angles_current_sim))
                    vel_improvement = np.sum(np.abs(prev_vel)) - np.sum(np.abs(angular_velocities_current_sim))
                    reward += (euler_improvement + vel_improvement) * 50.0

                # 3. å¿«é€Ÿæ”¶æ•›å¥–åŠ±ï¼ˆåˆ°å°çš„è¿‡ç¨‹è¶Šå¿«è¶Šå¥½ï¼‰
                convergence_bonus = 1.0 / (step + 1)  # éšæ—¶é—´é€’å‡çš„å¥–åŠ±
                reward += convergence_bonus * 100.0

                # 4. è¾“å…¥å˜åŒ–ç‡æƒ©ç½šï¼ˆå˜åŒ–è¶Šæ…¢è¶Šå¥½ï¼‰
                if step > 0:
                    input_change = np.sum(np.abs(next_input_array_sim - prev_input_array_sim))
                    reward -= input_change * 5.0

                # ä¿æŒåŸæœ‰çš„ç¨³å®šæ€§å¥–åŠ±
                if np.all(np.abs(euler_angles_current_sim) <= 1.0):
                    reward += XY_STABLE_REWARD
                if np.all(np.abs(angular_velocities_current_sim) <= 30.0):
                    reward += ANGULAR_VEL_STABLE_REWARD

            # æ£€æŸ¥å¥–åŠ±å€¼æ˜¯å¦ä¸ºNaNï¼ˆæ— æ•ˆå€¼ï¼‰
            if np.isnan(reward):
                # è®¾ç½®æƒ©ç½šå¥–åŠ±
                reward = nan_penalty
                # æ ‡è®°å½“å‰æ­¥ä¸ºç»ˆæ­¢çŠ¶æ€
                # å¦‚æœå½“å‰æ­¥æ˜¯ç»ˆæ­¢æ­¥ï¼Œåˆ™è®¾ç½®å½“å‰æ­¥ä¸ºç»ˆæ­¢
                current_step_is_terminal_worker = True

            # æå–å½“å‰è¾“å‡ºæ•°ç»„ä¸­çš„æ¬§æ‹‰è§’
            euler_angles_term_check = current_output_array[3:]

            # è®¡ç®—å½“å‰æ¬§æ‹‰è§’ä¸å‰ä¸€æ—¶åˆ»æ¬§æ‹‰è§’ä¹‹é—´çš„è§’é€Ÿåº¦
            angular_velocities_term_check = (euler_angles_term_check - prev_output_array_sim[3:]) / time_step

            # æ£€æŸ¥æ¬§æ‹‰è§’å’Œè§’é€Ÿåº¦æ˜¯å¦è¶…å‡ºé˜ˆå€¼
            if (np.any(np.abs(euler_angles_term_check) > euler_deadzone) or 
                np.any(np.abs(angular_velocities_term_check) > angular_velocity_deadzone)):
                # å¦‚æœå½“å‰æ­¥ä¸æ˜¯ç»ˆæ­¢æ­¥ï¼Œåˆ™è®¾ç½®å¥–åŠ±ä¸ºnan_penalty
                if not current_step_is_terminal_worker:
                    reward = nan_penalty
                # è®¾ç½®å½“å‰æ­¥ä¸ºç»ˆæ­¢
                current_step_is_terminal_worker = True

            # åˆ¤æ–­æ˜¯å¦å®Œæˆå½“å‰é›†æ•°
            done = current_step_is_terminal_worker or (step == steps_per_episode - 1)

            # æ›´æ–°ä¸‹ä¸€ä¸ªæ¬§æ‹‰è§’
            next_euler_angles = current_output_array[3:]

            # æ›´æ–°ä¸‹ä¸€ä¸ªè§’é€Ÿåº¦
            next_angular_velocities = angular_velocities_current_sim

            # æ„å»ºä¸‹ä¸€ä¸ªçŠ¶æ€
            next_state = np.concatenate([
                next_input_array_sim[0], next_euler_angles, next_angular_velocities
            ]).flatten()

            # æ£€æŸ¥ä¸‹ä¸€ä¸ªçŠ¶æ€æ˜¯å¦åŒ…å«NaNå€¼
            if np.isnan(next_state).any() and not done:
                # å¦‚æœä¸‹ä¸€ä¸ªçŠ¶æ€åŒ…å«NaNå€¼ä¸”å½“å‰æ­¥æœªå®Œæˆï¼Œåˆ™è®¾ç½®å¥–åŠ±ä¸ºnan_penalty
                reward = nan_penalty 
                # æ ‡è®°å½“å‰æ­¥ä¸ºå®Œæˆ
                done = True

            # è®°å½•å½“å‰è¾“å…¥åŠ¨ä½œ
            episode_input_actions.append(copy.deepcopy(next_input_array_sim[0]))

            try:
                # å¦‚æœå½“å‰æ­¥å·²å®Œæˆ
                if done:
                     # ğŸ“Œã€æ—¥å¿—ä½ç½® 4ã€‘ï¼šEpisode ç»“æŸæ—¶æ‰“å°å®Œæ•´æ¬§æ‹‰è§’æ•°æ®
                    #print(f"[Worker {env_id}] Episode {episode_num} çš„æ¬§æ‹‰è§’æ•°æ® (é•¿åº¦: {len(episode_euler_angles)}):")
                    #print(np.array(episode_euler_angles))
                    # å°†å½“å‰ç»éªŒæ•°æ®æ”¾å…¥ç»éªŒé˜Ÿåˆ—
                    # å°†å½“å‰ç»éªŒæ•°æ®æ”¾å…¥ç»éªŒé˜Ÿåˆ—ï¼ŒåŒ…æ‹¬çŠ¶æ€ã€åŠ¨ä½œã€åŠ¨ä½œå¯¹æ•°æ¦‚ç‡ã€å¥–åŠ±ã€ä¸‹ä¸€ä¸ªçŠ¶æ€ã€æ˜¯å¦å®Œæˆã€ç¯å¢ƒIDã€æ¬§æ‹‰è§’å’Œè§’é€Ÿåº¦
                    experience_queue.put({
                        'state': state,
                        'action': action,
                        'action_log_prob': action_log_prob,
                        'reward': reward,
                        'next_state': next_state,
                        'done': done,
                        'env_id': env_id,
                        'euler_angles': np.array(episode_euler_angles),
                        'angular_velocities': np.array(episode_angular_velocities),
                        'input_data': np.array(episode_input_actions)  # æ·»åŠ  input æ•°æ®
                    })
                else:
                    # å¦‚æœå½“å‰æ­¥æœªå®Œæˆï¼Œåˆ™å°†å½“å‰ç»éªŒæ•°æ®æ”¾å…¥ç»éªŒé˜Ÿåˆ—
                    experience_queue.put((state, action, action_log_prob, reward, next_state, done, env_id))
                    # ğŸ“Œã€æ—¥å¿—ä½ç½® 1ã€‘ï¼šWorker å‘é€æ•°æ®å‰æ‰“å°å®Œæ•´æ¬§æ‹‰è§’æ•°æ®
                #print(f"[Worker {env_id}] Episode {episode_num} çš„æ¬§æ‹‰è§’æ•°æ® (é•¿åº¦: {len(episode_euler_angles)}):")
                #print(np.array(episode_euler_angles))  # æ‰“å°å®Œæ•´æ•°æ®
            except Exception as e:
                # æ‰“å°å‘é€ç»éªŒå¤±è´¥çš„é”™è¯¯ä¿¡æ¯
                print(f"[Worker {env_id}] å‘é€ç»éªŒå¤±è´¥: {e}")
                # é€€å‡ºå½“å‰å¾ªç¯
                break

            # æ›´æ–°å½“å‰çŠ¶æ€ä¸ºä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œä»¥ä¾¿åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ä½¿ç”¨
            state = next_state.copy()  # å°†ä¸‹ä¸€ä¸ªçŠ¶æ€å¤åˆ¶ç»™å½“å‰çŠ¶æ€

            # æ›´æ–°å½“å‰è¾“å…¥æ•°ç»„ä¸ºä¸‹ä¸€ä¸ªè¾“å…¥æ•°ç»„ï¼Œä»¥ä¾¿åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ä½¿ç”¨
            current_input_array = next_input_array_sim.copy()  # å°†ä¸‹ä¸€ä¸ªè¾“å…¥æ•°ç»„å¤åˆ¶ç»™å½“å‰è¾“å…¥æ•°ç»„

            # æ›´æ–°å‰ä¸€ä¸ªè¾“å‡ºæ•°ç»„ä¸ºå½“å‰è¾“å‡ºæ•°ç»„ï¼Œä»¥ä¾¿åœ¨ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥ä½¿ç”¨
            prev_output_array_sim = current_output_array.copy()  # å°†å½“å‰è¾“å‡ºæ•°ç»„å¤åˆ¶ç»™å‰ä¸€ä¸ªè¾“å‡ºæ•°ç»„

            # æ›´æ–°å½“å‰æ—¶é—´æ­¥
            current_time_sim += time_step  # å¢åŠ å½“å‰æ—¶é—´æ­¥çš„æ—¶é—´

            # å¦‚æœå½“å‰é›†æ•°å·²å®Œæˆï¼Œåˆ™è·³å‡ºå¾ªç¯
            if done:
                break  # ç»“æŸå½“å‰é›†æ•°çš„å¾ªç¯

        # æ‰“å°å½“å‰å·¥ä½œè¿›ç¨‹çš„åœæ­¢ä¿¡æ¯
        print(f"[Worker {env_id}] åœæ­¢.")  # è¾“å‡ºå½“å‰å·¥ä½œè¿›ç¨‹çš„åœæ­¢ä¿¡æ¯

if __name__ == '__main__':
    try:
        # è®¾ç½®å¤šè¿›ç¨‹çš„å¯åŠ¨æ–¹æ³•ä¸º 'spawn'ï¼Œä»¥ç¡®ä¿å­è¿›ç¨‹çš„ç‹¬ç«‹æ€§
        mp.set_start_method('spawn', force=True)
        print("Multiprocessing start method set to 'spawn'.")  # è¾“å‡ºå¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•è®¾ç½®æˆåŠŸçš„ä¿¡æ¯
    except RuntimeError as e:
        # å¦‚æœè®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•æ—¶å‘ç”Ÿé”™è¯¯ï¼Œåˆ™è¾“å‡ºé”™è¯¯ä¿¡æ¯
        print(f"Note: Multiprocessing start method already set or error: {e}")  # è¾“å‡ºé”™è¯¯ä¿¡æ¯æˆ–æç¤ºå¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•å·²è®¾ç½®

    # è®¡ç®—çŠ¶æ€ç»´åº¦ï¼ŒåŒ…æ‹¬å¸¸é‡è¾“å…¥ã€æ¬§æ‹‰è§’å’Œè§’é€Ÿåº¦
    state_dim_calc = INPUT_SIZE_CONST + 3 + 3  # çŠ¶æ€ç»´åº¦ç­‰äºå¸¸é‡è¾“å…¥ç»´åº¦åŠ ä¸Šæ¬§æ‹‰è§’å’Œè§’é€Ÿåº¦çš„ç»´åº¦

    # è®¡ç®—åŠ¨ä½œç»´åº¦ï¼Œå³å¸¸é‡è¾“å…¥çš„ç»´åº¦
    action_dim_calc = INPUT_SIZE_CONST  # åŠ¨ä½œç»´åº¦ç­‰äºå¸¸é‡è¾“å…¥çš„ç»´åº¦

    # åˆå§‹åŒ–PPOæ§åˆ¶å™¨
    ppo_controller = PPOController(
        state_dim=state_dim_calc,  # è®¾ç½®çŠ¶æ€ç»´åº¦
        action_dim=action_dim_calc,  # è®¾ç½®åŠ¨ä½œç»´åº¦
        lr_actor=LR_ACTOR,  # è®¾ç½®ç­–ç•¥ç½‘ç»œçš„å­¦ä¹ ç‡
        lr_critic=LR_CRITIC,  # è®¾ç½®ä»·å€¼ç½‘ç»œçš„å­¦ä¹ ç‡
        gamma=GAMMA,  # è®¾ç½®æŠ˜æ‰£å› å­
        K_epochs=K_EPOCHS,  # è®¾ç½®ç­–ç•¥æ›´æ–°çš„è½®æ•°
        eps_clip=EPS_CLIP,  # è®¾ç½®ç­–ç•¥æ›´æ–°çš„å‰ªåˆ‡èŒƒå›´
        action_std_init=ACTION_STD_INIT,  # è®¾ç½®åŠ¨ä½œæ ‡å‡†å·®çš„åˆå§‹å€¼
        max_grad_norm=MAX_GRAD_NORM,  # è®¾ç½®æ¢¯åº¦çš„æœ€å¤§èŒƒæ•°
        device=device  # è®¾ç½®è¿è¡Œè®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
    )

    # è¾“å‡ºPPOæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆçš„ä¿¡æ¯
    print(f"PPOController initialized on {device}")  # è¾“å‡ºæ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆçš„ä¿¡æ¯

    # åˆ›å»ºç»éªŒé˜Ÿåˆ—ï¼Œç”¨äºå­˜å‚¨å„ä¸ªå·¥ä½œè¿›ç¨‹çš„ç»éªŒ
    experience_queue = mp.Queue()  # åˆ›å»ºä¸€ä¸ªç»éªŒé˜Ÿåˆ—

    # åˆ›å»ºåŠ¨ä½œé˜Ÿåˆ—ï¼Œç”¨äºå­˜å‚¨å„ä¸ªå·¥ä½œè¿›ç¨‹çš„åŠ¨ä½œ
    action_queues = [mp.Queue() for _ in range(NUM_ENVIRONMENTS)]  # åˆ›å»ºå¤šä¸ªåŠ¨ä½œé˜Ÿåˆ—ï¼Œæ¯ä¸ªå·¥ä½œè¿›ç¨‹ä¸€ä¸ª

    # åˆ›å»ºçŠ¶æ€é˜Ÿåˆ—ï¼Œç”¨äºå­˜å‚¨å„ä¸ªå·¥ä½œè¿›ç¨‹çš„çŠ¶æ€
    state_queues = [mp.Queue() for _ in range(NUM_ENVIRONMENTS)]  # åˆ›å»ºå¤šä¸ªçŠ¶æ€é˜Ÿåˆ—ï¼Œæ¯ä¸ªå·¥ä½œè¿›ç¨‹ä¸€ä¸ª

    # å®šä¹‰å·¥ä½œè¿›ç¨‹çš„å‚æ•°
    worker_args = {
        'dll_path': DLL_PATH,  # DLLæ–‡ä»¶è·¯å¾„
        'steps_per_episode': STEPS_PER_EPISODE,  # æ¯ä¸ªé›†æ•°çš„æ­¥æ•°
        'time_step': TIME_STEP,  # æ¯ä¸ªæ—¶é—´æ­¥çš„æ—¶é—´
        'input_size': INPUT_SIZE_CONST,  # è¾“å…¥æ•°ç»„çš„å¤§å°
        'output_size': OUTPUT_SIZE_CONST,  # è¾“å‡ºæ•°ç»„çš„å¤§å°
        'nan_penalty': NAN_PENALTY,  # NaNå€¼çš„æƒ©ç½š
        'xy_angle_threshold': XY_ANGLE_THRESHOLD,  # æ¬§æ‹‰è§’é˜ˆå€¼
        'xy_stable_reward': XY_STABLE_REWARD,  # æ¬§æ‹‰è§’ç¨³å®šå¥–åŠ±
        'angular_vel_threshold': ANGULAR_VEL_THRESHOLD,  # è§’é€Ÿåº¦é˜ˆå€¼
        'angular_vel_stable_reward': ANGULAR_VEL_STABLE_REWARD,  # è§’é€Ÿåº¦ç¨³å®šå¥–åŠ±
        'euler_deadzone': EULER_DEADZONE,  # æ¬§æ‹‰è§’æ­»åŒº
        'angular_velocity_deadzone': ANGULAR_VELOCITY_DEADZONE,  # è§’é€Ÿåº¦æ­»åŒº
        'max_output_value': MAX_OUTPUT_VALUE  # è¾“å‡ºå€¼çš„æœ€å¤§å€¼
    }

    # åˆ›å»ºè¿›ç¨‹åˆ—è¡¨
    processes = []  # åˆå§‹åŒ–è¿›ç¨‹åˆ—è¡¨

    # åˆ›å»ºæŒ‡å®šæ•°é‡çš„å·¥ä½œè¿›ç¨‹
    # éå†ç¯å¢ƒæ•°é‡ï¼Œåˆ›å»ºç›¸åº”æ•°é‡çš„å·¥ä½œè¿›ç¨‹
    for i in range(NUM_ENVIRONMENTS):  # éå†ç¯å¢ƒæ•°é‡ï¼Œåˆ›å»ºç›¸åº”æ•°é‡çš„å·¥ä½œè¿›ç¨‹
        p = mp.Process(target=run_single_environment, args=(i, experience_queue, action_queues, state_queues, worker_args))  # åˆ›å»ºå·¥ä½œè¿›ç¨‹
        processes.append(p)  # å°†å·¥ä½œè¿›ç¨‹æ·»åŠ åˆ°è¿›ç¨‹åˆ—è¡¨ä¸­
        p.start()  # å¯åŠ¨å·¥ä½œè¿›ç¨‹
        print(f"Worker {i} started.")  # æ‰“å°å·¥ä½œè¿›ç¨‹å¯åŠ¨ä¿¡æ¯

    collected_timesteps_since_update = 0  # åˆå§‹åŒ–è‡ªä¸Šæ¬¡æ›´æ–°ä»¥æ¥æ”¶é›†çš„æ—¶é—´æ­¥æ•°
    total_episodes_completed_main = 0  # åˆå§‹åŒ–ä¸»è¿›ç¨‹å®Œæˆçš„æ€»é›†æ•°
    current_episode_rewards_agg = [0.0] * NUM_ENVIRONMENTS  # åˆå§‹åŒ–å½“å‰é›†çš„å¥–åŠ±èšåˆåˆ—è¡¨
    current_episode_steps_agg = [0] * NUM_ENVIRONMENTS  # åˆå§‹åŒ–å½“å‰é›†çš„æ—¶é—´æ­¥èšåˆåˆ—è¡¨

    try:
        while main_process_total_steps < TOTAL_TRAINING_STEPS:  # å½“ä¸»è¿›ç¨‹æ€»æ—¶é—´æ­¥æ•°å°äºæ€»è®­ç»ƒæ—¶é—´æ­¥æ•°æ—¶ï¼Œç»§ç»­å¾ªç¯
            for i in range(NUM_ENVIRONMENTS):  # éå†ç¯å¢ƒæ•°é‡
                if not state_queues[i].empty():  # æ£€æŸ¥çŠ¶æ€é˜Ÿåˆ—æ˜¯å¦ä¸ºç©º
                    state_np = state_queues[i].get()  # ä»çŠ¶æ€é˜Ÿåˆ—ä¸­è·å–çŠ¶æ€
                    state_tensor = torch.FloatTensor(state_np).to(device)  # å°†çŠ¶æ€è½¬æ¢ä¸ºå¼ é‡å¹¶ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                    action_tensor, action_log_prob_tensor = ppo_controller.select_action(state_tensor)  # é€‰æ‹©åŠ¨ä½œå¹¶è·å–åŠ¨ä½œå¯¹æ•°æ¦‚ç‡
                    action_np = action_tensor.detach().cpu().numpy()  # å°†åŠ¨ä½œå¼ é‡è½¬æ¢ä¸ºnumpyæ•°ç»„
                    action_log_prob_np = action_log_prob_tensor.detach().cpu().numpy()  # å°†åŠ¨ä½œå¯¹æ•°æ¦‚ç‡å¼ é‡è½¬æ¢ä¸ºnumpyæ•°ç»„
                    action_queues[i].put((action_np, action_log_prob_np))  # å°†åŠ¨ä½œå’ŒåŠ¨ä½œå¯¹æ•°æ¦‚ç‡æ”¾å…¥åŠ¨ä½œé˜Ÿåˆ—

            while not experience_queue.empty():  # å½“ç»éªŒé˜Ÿåˆ—ä¸ä¸ºç©ºæ—¶ï¼Œç»§ç»­å¾ªç¯
                experience = experience_queue.get()  # ä»ç»éªŒé˜Ÿåˆ—ä¸­è·å–ç»éªŒ
                if isinstance(experience, dict):  # æ£€æŸ¥ç»éªŒæ˜¯å¦ä¸ºå­—å…¸ç±»å‹
                    state = experience['state']  # æå–çŠ¶æ€
                    action = experience['action']  # æå–åŠ¨ä½œ
                    log_prob = experience['action_log_prob']  # æå–åŠ¨ä½œå¯¹æ•°æ¦‚ç‡
                    reward = experience['reward']  # æå–å¥–åŠ±
                    next_state = experience['next_state']  # æå–ä¸‹ä¸€ä¸ªçŠ¶æ€
                    done = experience['done']  # æå–æ˜¯å¦å®Œæˆæ ‡å¿—
                    worker_id = experience['env_id']  # æå–å·¥ä½œè¿›ç¨‹ID
                    euler_angles_data = experience['euler_angles']  # æå–æ¬§æ‹‰è§’æ•°æ®
                    angular_velocities_data = experience['angular_velocities']  # æå–è§’é€Ÿåº¦æ•°æ®
                    input_data = experience['input_data']  # æå– input æ•°æ®

                    # ğŸ“Œã€æ—¥å¿—ä½ç½® 5ã€‘ï¼šä¸»è¿›ç¨‹ä¸­æ¥æ”¶æ•°æ®åæ‰“å°æ¬§æ‹‰è§’æ•°æ®
                    # print(f"Episode {total_episodes_completed_main} çš„æ¬§æ‹‰è§’æ•°æ® (å½¢çŠ¶: {euler_angles_data.shape}):")
                    # print(euler_angles_data[:5])  # æ‰“å°å‰5è¡Œæ•°æ®

                    if total_episodes_completed_main % SAVE_PLOT_EVERY_EPISODES == 0:  # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜ç»˜å›¾
                       output_dir = os.path.join(PLOT_DIR, 'episode_outputs')  # æ„å»ºè¾“å‡ºç›®å½•è·¯å¾„
                       os.makedirs(output_dir, exist_ok=True)  # åˆ›å»ºè¾“å‡ºç›®å½•ï¼Œå¦‚æœç›®å½•å·²å­˜åœ¨åˆ™ä¸æŠ¥é”™

                        # ä¿å­˜æ¬§æ‹‰è§’æ•°æ®åˆ° CSV æ–‡ä»¶
                       euler_csv_path = os.path.join(output_dir, f'euler_angles_ep_{total_episodes_completed_main}.csv')  # æ„å»ºæ¬§æ‹‰è§’æ•°æ®æ–‡ä»¶è·¯å¾„
                       np.savetxt(euler_csv_path, euler_angles_data, delimiter=',',            
                       header='Roll,Pitch,Yaw', comments='')  # å°†æ¬§æ‹‰è§’æ•°æ®ä¿å­˜åˆ° CSV æ–‡ä»¶

                        # ä¿å­˜è§’é€Ÿåº¦æ•°æ®åˆ° CSV æ–‡ä»¶
                       velocity_csv_path = os.path.join(output_dir, f'angular_velocities_ep_{total_episodes_completed_main}.csv')  # æ„å»ºè§’é€Ÿåº¦æ•°æ®æ–‡ä»¶è·¯å¾„
                       np.savetxt(velocity_csv_path, angular_velocities_data, delimiter=',',            
                       header='Roll Rate,Pitch Rate,Yaw Rate', comments='')  # å°†è§’é€Ÿåº¦æ•°æ®ä¿å­˜åˆ° CSV æ–‡ä»¶

                        # ä¿å­˜ input æ•°æ®åˆ° CSV æ–‡ä»¶
                       input_csv_path = os.path.join(output_dir, f'input_data_ep_{total_episodes_completed_main}.csv')
                       np.savetxt(input_csv_path, input_data, delimiter=',',
                                  header='Input1,Input2,Input3,Input4,Input5,Input6', comments='')

                       print(f"Episode {total_episodes_completed_main} çš„æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")  # æ‰“å°æ•°æ®ä¿å­˜è·¯å¾„

                       # ğŸ“Œã€æ—¥å¿—ä½ç½® 6ã€‘ï¼šä¿å­˜å›¾åƒå’Œ CSV å‰æ‰“å°æ¬§æ‹‰è§’æ•°æ®
                    # print(f"ä¿å­˜ Episode {total_episodes_completed_main} çš„æ¬§æ‹‰è§’æ•°æ® (å½¢çŠ¶: {euler_angles_data.shape}):")
                    # print(euler_angles_data[:5])
                else:
                    state, action, log_prob, reward, next_state, done, worker_id = experience  # ä»ç»éªŒé˜Ÿåˆ—ä¸­è·å–ç»éªŒæ•°æ®

                ppo_controller.buffer.add(state, action, log_prob, reward, next_state, done)  # å°†ç»éªŒæ•°æ®æ·»åŠ åˆ° PPO æ§åˆ¶å™¨çš„ç¼“å†²åŒº
                current_episode_rewards_agg[worker_id] += reward  # æ›´æ–°å½“å‰å·¥ä½œè¿›ç¨‹çš„å¥–åŠ±
                current_episode_steps_agg[worker_id] += 1  # æ›´æ–°å½“å‰å·¥ä½œè¿›ç¨‹çš„æ­¥æ•°
                main_process_total_steps += 1  # æ›´æ–°ä¸»è¿›ç¨‹çš„æ€»æ­¥æ•°
                collected_timesteps_since_update += 1  # æ›´æ–°è‡ªä¸Šæ¬¡æ›´æ–°ä»¥æ¥æ”¶é›†çš„æ—¶é—´æ­¥æ•°

                if done:  # å¦‚æœå½“å‰é›†æ•°ç»“æŸ
                    total_episodes_completed_main += 1  # æ›´æ–°å®Œæˆçš„é›†æ•°
                    all_episode_rewards.append(current_episode_rewards_agg[worker_id])  # è®°å½•å½“å‰é›†æ•°çš„å¥–åŠ±
                    avg_reward = np.mean(all_episode_rewards[-50:])  # è®¡ç®—æœ€è¿‘50ä¸ªé›†æ•°çš„å¹³å‡å¥–åŠ±
                    print(f"Total Steps: {main_process_total_steps}/{TOTAL_TRAINING_STEPS}, Worker {worker_id} Episode Finished. Reward: {current_episode_rewards_agg[worker_id]:.2f}, Steps: {current_episode_steps_agg[worker_id]}, Avg Reward (last 50): {avg_reward:.2f}")  # æ‰“å°å½“å‰é›†æ•°çš„è®­ç»ƒä¿¡æ¯
                    
                    # ä¿å­˜å¹³å‡å¥–åŠ±æ•°æ®åˆ°CSVæ–‡ä»¶
                    avg_rewards_file = os.path.join(PLOT_DIR, "avg_rewards_all.csv")
                    with open(avg_rewards_file, 'a') as f:
                        f.write(f"{total_episodes_completed_main},{avg_reward}\n")
                    current_episode_rewards_agg[worker_id] = 0.0  # é‡ç½®å½“å‰é›†æ•°çš„å¥–åŠ±
                    current_episode_steps_agg[worker_id] = 0  # é‡ç½®å½“å‰é›†æ•°çš„æ­¥æ•°

            if collected_timesteps_since_update >= UPDATE_TIMESTEPS:  # å¦‚æœæ”¶é›†çš„ç»éªŒæ­¥æ•°è¾¾åˆ°æ›´æ–°é˜ˆå€¼
                print(f"Total Steps: {main_process_total_steps}. Updating PPO policy with {len(ppo_controller.buffer.rewards)} experiences...")  # æ‰“å°æ›´æ–°ä¿¡æ¯
                ppo_controller.update()  # æ›´æ–°PPOç­–ç•¥
                collected_timesteps_since_update = 0  # é‡ç½®æ”¶é›†çš„ç»éªŒæ­¥æ•°
                print("PPO policy updated.")  # æ‰“å°æ›´æ–°å®Œæˆä¿¡æ¯

            if main_process_total_steps >= TOTAL_TRAINING_STEPS:  # å¦‚æœè¾¾åˆ°æ€»è®­ç»ƒæ­¥æ•°
                print("è¾¾åˆ°æ€»è®­ç»ƒæ­¥æ•°ï¼Œåœæ­¢è®­ç»ƒã€‚")  # æ‰“å°åœæ­¢è®­ç»ƒä¿¡æ¯
                break  # ç»“æŸè®­ç»ƒ

            time.sleep(0.001)  # çŸ­æš‚ä¼‘çœ ï¼Œé˜²æ­¢CPUå ç”¨è¿‡é«˜

    except KeyboardInterrupt:  # æ•è·é”®ç›˜ä¸­æ–­ä¿¡å·
        print("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")  # æ‰“å°ä¸­æ–­ä¿¡æ¯

    finally:  # æ— è®ºæ˜¯å¦å‘ç”Ÿå¼‚å¸¸ï¼Œæ‰§è¡Œä»¥ä¸‹ä»£ç 
        print("è®¾ç½®åœæ­¢ä¿¡å·ï¼Œç­‰å¾…å·¥ä½œè¿›ç¨‹ç»“æŸ...")  # æ‰“å°è¿›ç¨‹ç»“æŸä¿¡æ¯
        for p in processes:  # éå†æ‰€æœ‰å·¥ä½œè¿›ç¨‹
            p.terminate()  # å‘é€ç»ˆæ­¢ä¿¡å·
            p.join()  # ç­‰å¾…è¿›ç¨‹ç»“æŸ
        print("æ‰€æœ‰å·¥ä½œè¿›ç¨‹å·²ç»“æŸã€‚")  # æ‰“å°æ‰€æœ‰è¿›ç¨‹ç»“æŸä¿¡æ¯

    plt.figure(figsize=(12, 6))  # åˆ›å»ºä¸€ä¸ªæ–°çš„å›¾å½¢çª—å£
    plt.plot(all_episode_rewards, label='æ€»å¥–åŠ±')  # ç»˜åˆ¶æ‰€æœ‰é›†æ•°çš„å¥–åŠ±æ›²çº¿

    # è®¡ç®—æœ€è¿‘ 50 ä¸ª episode çš„å¹³å‡å¥–åŠ±
    if len(all_episode_rewards) >= 50:  # å¦‚æœé›†æ•°å¤§äºç­‰äº50
        avg_reward = np.mean(all_episode_rewards[-50:])  # è®¡ç®—æœ€è¿‘50ä¸ªé›†æ•°çš„å¹³å‡å¥–åŠ±
    else:
        avg_reward = np.mean(all_episode_rewards)  # å¦åˆ™è®¡ç®—æ‰€æœ‰é›†æ•°çš„å¹³å‡å¥–åŠ±

    # ä¿å­˜å¹³å‡å¥–åŠ±åˆ° CSV
    reward_summary_path = os.path.join(output_dir, 'average_rewards.csv')  # æ„å»ºCSVæ–‡ä»¶è·¯å¾„
    with open(reward_summary_path, 'a') as f:  # æ‰“å¼€CSVæ–‡ä»¶ï¼Œè¿½åŠ æ¨¡å¼
        if os.path.getsize(reward_summary_path) == 0:  # å¦‚æœæ–‡ä»¶ä¸ºç©º
            f.write('Episode,AvgReward50\n')  # å†™å…¥è¡¨å¤´
        f.write(f'{total_episodes_completed_main},{avg_reward:.2f}\n')  # å†™å…¥å½“å‰é›†æ•°å’Œå¹³å‡å¥–åŠ±

    plt.xlabel('æ€»è½®æ•° (æ‰€æœ‰Worker)')  # è®¾ç½®xè½´æ ‡ç­¾
    plt.ylabel('æ€»å¥–åŠ±')  # è®¾ç½®yè½´æ ‡ç­¾
    plt.title('æœ€ç»ˆå¤šè¿›ç¨‹PPOè®­ç»ƒå¥–åŠ±æ›²çº¿')  # è®¾ç½®å›¾è¡¨æ ‡é¢˜
    plt.legend()  # æ˜¾ç¤ºå›¾ä¾‹
    plt.grid(True)  # æ˜¾ç¤ºç½‘æ ¼

    final_reward_plot_path = os.path.join(PLOT_DIR, REWARD_PLOT_FILENAME)  # æ„å»ºå¥–åŠ±æ›²çº¿å›¾ä¿å­˜è·¯å¾„
    plt.savefig(final_reward_plot_path)  # ä¿å­˜å¥–åŠ±æ›²çº¿å›¾
    plt.close()  # å…³é—­å›¾è¡¨

    print(f"æœ€ç»ˆå¥–åŠ±æ›²çº¿å›¾å·²ä¿å­˜åˆ°: {final_reward_plot_path}")  # æ‰“å°ä¿å­˜è·¯å¾„

    final_model_path = os.path.join(PLOT_DIR, MODEL_SAVE_PATH)  # æ„å»ºæ¨¡å‹ä¿å­˜è·¯å¾„
    ppo_controller.save(final_model_path)  # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")  # æ‰“å°ä¿å­˜è·¯å¾„

    print("è®­ç»ƒå®Œæˆã€‚")  # æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯